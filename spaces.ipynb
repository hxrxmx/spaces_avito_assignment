{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75b040a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (21.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (6.0.2)\n",
      "Requirement already satisfied: razdel in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy pyarrow tqdm lxml razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc5c2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import tokenize\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "id": "9d0aaccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57755fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset_1937770_3.txt\", \"r\", encoding=\"utf8\") as in_file:\n",
    "    with open(\"dataset.txt\", \"w\", encoding=\"utf8\") as out_file:\n",
    "        for line in in_file.readlines():\n",
    "            out_file.write(line.replace(\",\", \";\", 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4237808b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20                 ищуподработкуповечерам\n",
       "71                   ищуработуофицианткой\n",
       "102          куплюковрикдляйоги,недорого!\n",
       "106    ищудрузейдляпутешествий,летомвгоры\n",
       "121         ищустоликдляноутбука,складной\n",
       "270                     куплюгитаруGibson\n",
       "435               Когданачасахважныминуты\n",
       "614                       Верхомназвезде,\n",
       "700                     Ждутегодомавсегда\n",
       "860      Янивчемнеоткажу,янивчёмнеоткажу!\n",
       "Name: text_no_spaces, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"dataset.txt\", sep=\";\", engine=\"python\", on_bad_lines=\"skip\")\n",
    "\n",
    "random_idxs = np.random.randint(0, 1000, 10)\n",
    "\n",
    "data[\"text_no_spaces\"][np.isin(data[\"id\"], random_idxs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba50b6be",
   "metadata": {},
   "source": [
    "Во время экспериментов пробовался более продвинутый токенайзер, который разделяет по смысловым единицам, а не пробелам и символам, но на сыром тексте без пробелов он показывает себя не очень хорошо. В связи с этим, используется более простой токенайзер, который, по сути, полезен только чтоб отделять символы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "793ad3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['куплюковрикдляйоги', ',', 'недорого', '!']\n",
      "['Когданачасахважныминуты']\n",
      "['Янивчемнеоткажу', ',', 'янивчёмнеоткажу', '!']\n",
      "['куплюгитару', 'Gibson']\n",
      "['ищудрузейдляпутешествий', ',', 'летомвгоры']\n",
      "['ищуработуофицианткой']\n",
      "['Ждутегодомавсегда']\n",
      "['ищуподработкуповечерам']\n",
      "['Верхомназвезде', ',']\n",
      "['ищустоликдляноутбука', ',', 'складной']\n"
     ]
    }
   ],
   "source": [
    "corpus = data[\"text_no_spaces\"]\n",
    "\n",
    "for text in corpus[random_idxs]:\n",
    "\ttokens = [token.text for token in tokenize(text)]\n",
    "\tprint(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3603a0",
   "metadata": {},
   "source": [
    "Токенайзером на глаз видно, что тексты делятся очень плохо. Кажется, не имеет большого смысла использовать его в качестве предобработки сырых данных, а лучше использовать другой подход. (причем, ладно бы он делал только лишние разбиения. Можно было бы тогда свести задачу к попытке склеить лишние разбиения. Но с такой работой, которая наблюдается, он и лишних разбиений много сделал, и не везде там, где надо, разбил)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e02137",
   "metadata": {},
   "source": [
    "У нас есть задача: поделить строки таким образом, чтобы разделенный текст был осмысленным, а значит наиболее вероятным с точки зрения встречаемости слов вместе.\n",
    "\n",
    "то есть, пусть есть строка \"куплюковрик\". Можно поделить ее огромным количеством способов пробелами. При этом разбиение \"куплю коврик\" должно быть более приоритетным, чем какое бы то ни было другое, например \"куп люков рик\". Приоритетность отражается большей вероятностью встречи слов в датасете с запросами пользователей. Пусть разбиение текста запроса без пробелов -- это слова через пробел $x = \\{x_1, .., x_n\\}$. Тогда истинное разбиение $x^* = \\{x_1^*, .., x_k^*\\}$ должно быть более вероятное (скорее всего не всегда и можно привести примеры, но приблизительно должно выполняться):\n",
    "\n",
    "$$\n",
    "\tp(x^*) \\gtrsim p(x) \\quad \\forall x \\neq x^*,\n",
    "$$\n",
    "\n",
    "где\n",
    "$$\n",
    "\\begin{split}\n",
    "\tp(x^*) \\equiv p(x_1^*, ... , x_k^*) = p(x_1^*) \\cdot p(x_2^* | x_1^*) \\cdot ... \\cdot p(x_k^* | x_1^*, ... x_{k-1}^*), \\\\\n",
    "\tp(x) \\equiv p(x_1, ... , x_n) = .... \\text{аналогично}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Таким образом, зная все условные вероятности встретить некоторое слово $y_1$ в контексте других слов $\\{y_i\\}_{i=2}^{m}$ в текстах пользовательских запросов, можно разрешить эту задачу.\n",
    "\n",
    "Естественно, знать все такие вероятности невозможно, поэтому нужно прибегнуть к некоторым упрощениям.\n",
    "\n",
    "Первое допущение типа марковского. Считаем, что для больших последовательностей слова из начала не сильно влияют на вероятность слов с конца, то есть, например, контекста $l$ слов перед исходным с хорошей точностью хватит для оценки, поскольку $p(x_i | x_{i-1}, x_{i-2}, ... , x_{1}) \\approx p(x_i | \\underbrace{x_{i-1}, x_{i-2}, ... , x_{i-l}, x_{i-l - 1}}_{l})$\n",
    "\n",
    "На самом деле даже подход с $l=0$ (униграммы) отсеит много мусора, так как неправильное разбиение, в котором участвует слово \"рик\" из-за одной только вероятности $p(\"рик\") \\approx 0$ сведет к малому значению или нулю вероятность всего разбиения $p(\"куп\", \"люков\", \"рик\") \\approx p(\"куп\") \\cdot p(\"люков\") \\cdot p(\"рик\") \\approx 0$. Вероятность же просто адекватного разбиения на осмысленные слова будет больше этого.\n",
    "\n",
    "Однако, было бы здорово из двух адекватных разбиений (которые, вероятно, могут все таки встречаться) выбирать более подходящее по смыслу именно к запросам пользователей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00694fc8",
   "metadata": {},
   "source": [
    "Самое сложное в этом всем -- найти открытый датасет, который по статистике был бы похож на пользовательские запросы, названия и описания со словами, написанными транслитом (\"айфон\" или \"оппо\") и подобными артефактами."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d282f4",
   "metadata": {},
   "source": [
    "Вроде то, что надо: https://www.kaggle.com/datasets/antonoof/avito-data. Нет запросов правда, только названия и описания объявлений, но что-то лучше вряд ли получиться найти. Также скачаем корпус русскоязычных текстов: https://www.opencorpora.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85a10acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install kagglehub[pandas-datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1cf99d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "# from kagglehub import KaggleDatasetAdapter\n",
    "# from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efcdb24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"avito_data.csv\"\n",
    "\n",
    "# df = kagglehub.dataset_load(\n",
    "# \tKaggleDatasetAdapter.PANDAS,\n",
    "# \t\"antonoof/avito-data\",\n",
    "# \tfile_path,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6be6a0c",
   "metadata": {},
   "source": [
    "*НЕ РАБОТАЕТ, ВЫДАЕТ SSLEOFError*\n",
    "\n",
    "*ПРИ ПРОВЕРОЧНОМ ЗАПУСКЕ СКАЧАЙТЕ ДАТАСЕТЫ ВРУЧНУЮ И ПОМЕСТИТЕ АРХИВЫ РЯДОМ С НОУТБУКОМ*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3ff278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import zipfile\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2f10420",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = Path('archive.zip')\n",
    "extract_path = Path('avito_data')\n",
    "extract_path.mkdir(exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e28b31",
   "metadata": {},
   "source": [
    "Нужно сильно постараться, чтоб статистика по словам в таком датасете была похожа на статистику в словах реальных пользователей, ведь тут намеренно приведено много примеров очень похожих, использующих одинаковые слова. Однако, фильтровать все тексты, похожие по индексу Жаккара, например, будет очень долго. Поэтому, в трейновой выборке удалим просто примеры, у которых дублируется id и примеры, где стоит флаг is_double, а в тестовой выборке удалим просто всех кандидатов и объявления с одинаковым id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd77ed37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:16<00:00,  4.02s/it]\n"
     ]
    }
   ],
   "source": [
    "train_files = list((extract_path / \"data\").glob(\"train_part_*.snappy.parquet\"))\n",
    "train_cols = ['base_item_id', 'cand_item_id', 'is_double', 'base_title', 'cand_title', 'base_description', 'cand_description']\n",
    "train_df = pd.concat([pd.read_parquet(f, columns=train_cols) for f in tqdm(train_files)], ignore_index=True)\n",
    "\n",
    "doubles_df = train_df[train_df['is_double'] == 1]\n",
    "blacklisted_ids = set(doubles_df['base_item_id']).union(set(doubles_df['cand_item_id']))\n",
    "\n",
    "base_train = train_df[['base_item_id', 'base_title', 'base_description']].rename(columns={'base_item_id': 'item_id', 'base_title': 'title', 'base_description': 'description'})\n",
    "cand_train = train_df[['cand_item_id', 'cand_title', 'cand_description']].rename(columns={'cand_item_id': 'item_id', 'cand_title': 'title', 'cand_description': 'description'})\n",
    "all_train_df = pd.concat([base_train, cand_train], ignore_index=True).drop_duplicates()\n",
    "\n",
    "cleaned_train_df = all_train_df[~all_train_df[\"item_id\"].isin(blacklisted_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deaf70ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "test_files = list((extract_path / \"data\").glob(\"test_part_*.snappy.parquet\"))\n",
    "test_cols = ['base_item_id', 'base_title', 'base_description']\n",
    "test_df = pd.concat([pd.read_parquet(f, columns=test_cols) for f in tqdm(test_files)], ignore_index=True)\n",
    "\n",
    "base_test = test_df.rename(columns={'base_item_id': 'item_id', 'base_title': 'title', 'base_description': 'description'})\n",
    "cleaned_test_df = base_test.drop_duplicates(subset=['item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb8b9d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([cleaned_train_df, cleaned_test_df], ignore_index=True).drop_duplicates(subset=['item_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fbebfb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Подголовники priora</td>\n",
       "      <td>В хоpошем cоcтоянии \\nЦенa зa штуку\\nПapa 900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Коcтюм для тaнцев</td>\n",
       "      <td>Пpодaю коcтюм для тaнцев фееpия, зaкaзaли нa п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Хоккейные тpуcы Easton Synergy 900 Sr L</td>\n",
       "      <td>🏒Хоккейные тpуcы Easton Synergy 900\\n\\n🔸Рaзмеp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Стульчик для коpмления</td>\n",
       "      <td>cтульчик в отличном cоcтоянии. 3 позы, можно п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Бюcтгaльтеp Intimissimi новый</td>\n",
       "      <td>Новый бюcтгaльтеp Intimissimi модель giada 80Б...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>Лыжные ботинки nordway 40</td>\n",
       "      <td>Пpодaю лыжные ботинки в идеaльном cоcтоянии.Рa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>Пpоклaдки для гpуди</td>\n",
       "      <td>В оcтaтке 19 шт.Кaждaя в индивидуaльной упaков...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>Сaлфеткa для pук и лицa мaхpa вт боpдовый по ц</td>\n",
       "      <td>Вид товapa:  Сaлфеткa\\nНaзнaчение : для pук и ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>Зaбоp cеткa 3D</td>\n",
       "      <td>Зaбоp cеткa 3d\\n\\n👍 3д cеткa в нaличии и под з...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>Лего Мaйнкpaфт Деpевня 778 детaлей</td>\n",
       "      <td>Лего Мaйнкpaфт Деpевня 778 детaлей</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              title  \\\n",
       "20                              Подголовники priora   \n",
       "71                                Коcтюм для тaнцев   \n",
       "102         Хоккейные тpуcы Easton Synergy 900 Sr L   \n",
       "106                          Стульчик для коpмления   \n",
       "121                   Бюcтгaльтеp Intimissimi новый   \n",
       "270                       Лыжные ботинки nordway 40   \n",
       "435                             Пpоклaдки для гpуди   \n",
       "614  Сaлфеткa для pук и лицa мaхpa вт боpдовый по ц   \n",
       "700                                  Зaбоp cеткa 3D   \n",
       "860              Лего Мaйнкpaфт Деpевня 778 детaлей   \n",
       "\n",
       "                                           description  \n",
       "20       В хоpошем cоcтоянии \\nЦенa зa штуку\\nПapa 900  \n",
       "71   Пpодaю коcтюм для тaнцев фееpия, зaкaзaли нa п...  \n",
       "102  🏒Хоккейные тpуcы Easton Synergy 900\\n\\n🔸Рaзмеp...  \n",
       "106  cтульчик в отличном cоcтоянии. 3 позы, можно п...  \n",
       "121  Новый бюcтгaльтеp Intimissimi модель giada 80Б...  \n",
       "270  Пpодaю лыжные ботинки в идеaльном cоcтоянии.Рa...  \n",
       "435  В оcтaтке 19 шт.Кaждaя в индивидуaльной упaков...  \n",
       "614  Вид товapa:  Сaлфеткa\\nНaзнaчение : для pук и ...  \n",
       "700  Зaбоp cеткa 3d\\n\\n👍 3д cеткa в нaличии и под з...  \n",
       "860                 Лего Мaйнкpaфт Деpевня 778 детaлей  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df[[\"title\", \"description\"]][np.isin(full_df.index, random_idxs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba390c45",
   "metadata": {},
   "source": [
    "Теперь надо сделать предобработку, удалить хотя бы указанные дубликаты и привести все слова к нижнему регистру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "410a4daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2516491"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e79df98",
   "metadata": {},
   "source": [
    "Перейдем ко второму датасету"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "900903de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2761b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = Path(\"annot.opcorpora.xml.zip\")\n",
    "extract_path = Path(\"open_corpora_data\")\n",
    "extract_path.mkdir(exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ff1fb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4007/4007 [00:00<00:00, 7164.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xml_corpus_0</td>\n",
       "      <td></td>\n",
       "      <td>\"Частный корреспондент\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xml_corpus_1</td>\n",
       "      <td></td>\n",
       "      <td>00021 Школа злословия «Школа злословия» учит п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xml_corpus_2</td>\n",
       "      <td></td>\n",
       "      <td>00022 Последнее восстание в Сеуле «Последнее в...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xml_corpus_3</td>\n",
       "      <td></td>\n",
       "      <td>00023 За кота - ответишь! За кота – ответишь! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xml_corpus_4</td>\n",
       "      <td></td>\n",
       "      <td>00024 Быстротечный кинороман Быстротечный кино...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4002</th>\n",
       "      <td>xml_corpus_4002</td>\n",
       "      <td></td>\n",
       "      <td>Мягкое нёбо Мягкое нёбо Мягкое нёбо или нёбная...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4003</th>\n",
       "      <td>xml_corpus_4003</td>\n",
       "      <td></td>\n",
       "      <td>Небо Небо Небо (производные небеса, небосвод) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4004</th>\n",
       "      <td>xml_corpus_4004</td>\n",
       "      <td></td>\n",
       "      <td>Звёздное небо Звёздное небо Звёздное небо — со...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4005</th>\n",
       "      <td>xml_corpus_4005</td>\n",
       "      <td></td>\n",
       "      <td>Диффузное излучение неба Диффузное излучение н...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4006</th>\n",
       "      <td>xml_corpus_4006</td>\n",
       "      <td></td>\n",
       "      <td>\" Черновики</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4007 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              item_id title                                        description\n",
       "0        xml_corpus_0                                  \"Частный корреспондент\"\n",
       "1        xml_corpus_1        00021 Школа злословия «Школа злословия» учит п...\n",
       "2        xml_corpus_2        00022 Последнее восстание в Сеуле «Последнее в...\n",
       "3        xml_corpus_3        00023 За кота - ответишь! За кота – ответишь! ...\n",
       "4        xml_corpus_4        00024 Быстротечный кинороман Быстротечный кино...\n",
       "...               ...   ...                                                ...\n",
       "4002  xml_corpus_4002        Мягкое нёбо Мягкое нёбо Мягкое нёбо или нёбная...\n",
       "4003  xml_corpus_4003        Небо Небо Небо (производные небеса, небосвод) ...\n",
       "4004  xml_corpus_4004        Звёздное небо Звёздное небо Звёздное небо — со...\n",
       "4005  xml_corpus_4005        Диффузное излучение неба Диффузное излучение н...\n",
       "4006  xml_corpus_4006                                              \" Черновики\n",
       "\n",
       "[4007 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_corpus_file = extract_path / \"annot.opcorpora.xml\"\n",
    "tree = ET.parse(xml_corpus_file)\n",
    "root = tree.getroot()\n",
    "\n",
    "xml_texts = []\n",
    "for text_element in tqdm(root.findall('.//text')):\n",
    "    name_text = text_element.get('name', '')\n",
    "    source_texts = [source.text for source in text_element.findall('.//source') if source.text]\n",
    "\n",
    "    full_text_parts = [name_text] + source_texts\n",
    "    full_text = \" \".join(part.strip() for part in full_text_parts if part and part.strip())\n",
    "    \n",
    "    if full_text:\n",
    "        xml_texts.append(full_text)\n",
    "\n",
    "data_for_df = {\n",
    "    'item_id': [f'xml_corpus_{i}' for i in range(len(xml_texts))],\n",
    "    'title': '',\n",
    "    'description': xml_texts\n",
    "}\n",
    "additional_df = pd.DataFrame(data_for_df)\n",
    "additional_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0adce503",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([full_df, additional_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedd3650",
   "metadata": {},
   "source": [
    "Вроде нормально, теперь сделаем из этого единый корпус текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96f90ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5aa8b125a3218ee10d56452d6e3fc6b05b92acae7e0710...</td>\n",
       "      <td>Коcтюм женcкий</td>\n",
       "      <td>Новый комплект</td>\n",
       "      <td>Коcтюм женcкий. Новый комплект</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9f9b68e75034a161c116e764605db3275b514213c04ec7...</td>\n",
       "      <td>Школьный коcтюм для мaльчикa 134</td>\n",
       "      <td>Школьный коcтюм нa мaльчикa чеpного цветa, paз...</td>\n",
       "      <td>Школьный коcтюм для мaльчикa 134. Школьный коc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>538538e5282be1b3aa4cad2feb0000cadc9d4d6602b955...</td>\n",
       "      <td>Иcлaмcкaя кapтинa из эпокcидной cмолы</td>\n",
       "      <td>Я pиcую иcлaмcкиe кapтины из эпокcидной cмoлы ...</td>\n",
       "      <td>Иcлaмcкaя кapтинa из эпокcидной cмолы. Я pиcую...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30f2136fc45172af7f66d4993132c05649c563c720dc52...</td>\n",
       "      <td>Оpигинaл Nike Air Jordan 1 High OG University ...</td>\n",
       "      <td>Мы зaключили договоp c официaльным диcтpибьюто...</td>\n",
       "      <td>Оpигинaл Nike Air Jordan 1 High OG University ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4afe50981f5dded76ad9c4357a1df34dcf28f6248e2ccf...</td>\n",
       "      <td>Кpоccовки Nike Air Jordan 1 Low OG Travis Scot...</td>\n",
       "      <td>Nike Air Jordan 1 Low OG Travis Scott Black Ph...</td>\n",
       "      <td>Кpоccовки Nike Air Jordan 1 Low OG Travis Scot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2520493</th>\n",
       "      <td>xml_corpus_4002</td>\n",
       "      <td></td>\n",
       "      <td>Мягкое нёбо Мягкое нёбо Мягкое нёбо или нёбная...</td>\n",
       "      <td>. Мягкое нёбо Мягкое нёбо Мягкое нёбо или нёбн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2520494</th>\n",
       "      <td>xml_corpus_4003</td>\n",
       "      <td></td>\n",
       "      <td>Небо Небо Небо (производные небеса, небосвод) ...</td>\n",
       "      <td>. Небо Небо Небо (производные небеса, небосвод...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2520495</th>\n",
       "      <td>xml_corpus_4004</td>\n",
       "      <td></td>\n",
       "      <td>Звёздное небо Звёздное небо Звёздное небо — со...</td>\n",
       "      <td>. Звёздное небо Звёздное небо Звёздное небо — ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2520496</th>\n",
       "      <td>xml_corpus_4005</td>\n",
       "      <td></td>\n",
       "      <td>Диффузное излучение неба Диффузное излучение н...</td>\n",
       "      <td>. Диффузное излучение неба Диффузное излучение...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2520497</th>\n",
       "      <td>xml_corpus_4006</td>\n",
       "      <td></td>\n",
       "      <td>\" Черновики</td>\n",
       "      <td>. \" Черновики</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2520498 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   item_id  \\\n",
       "0        5aa8b125a3218ee10d56452d6e3fc6b05b92acae7e0710...   \n",
       "1        9f9b68e75034a161c116e764605db3275b514213c04ec7...   \n",
       "2        538538e5282be1b3aa4cad2feb0000cadc9d4d6602b955...   \n",
       "3        30f2136fc45172af7f66d4993132c05649c563c720dc52...   \n",
       "4        4afe50981f5dded76ad9c4357a1df34dcf28f6248e2ccf...   \n",
       "...                                                    ...   \n",
       "2520493                                    xml_corpus_4002   \n",
       "2520494                                    xml_corpus_4003   \n",
       "2520495                                    xml_corpus_4004   \n",
       "2520496                                    xml_corpus_4005   \n",
       "2520497                                    xml_corpus_4006   \n",
       "\n",
       "                                                     title  \\\n",
       "0                                           Коcтюм женcкий   \n",
       "1                         Школьный коcтюм для мaльчикa 134   \n",
       "2                    Иcлaмcкaя кapтинa из эпокcидной cмолы   \n",
       "3        Оpигинaл Nike Air Jordan 1 High OG University ...   \n",
       "4        Кpоccовки Nike Air Jordan 1 Low OG Travis Scot...   \n",
       "...                                                    ...   \n",
       "2520493                                                      \n",
       "2520494                                                      \n",
       "2520495                                                      \n",
       "2520496                                                      \n",
       "2520497                                                      \n",
       "\n",
       "                                               description  \\\n",
       "0                                           Новый комплект   \n",
       "1        Школьный коcтюм нa мaльчикa чеpного цветa, paз...   \n",
       "2        Я pиcую иcлaмcкиe кapтины из эпокcидной cмoлы ...   \n",
       "3        Мы зaключили договоp c официaльным диcтpибьюто...   \n",
       "4        Nike Air Jordan 1 Low OG Travis Scott Black Ph...   \n",
       "...                                                    ...   \n",
       "2520493  Мягкое нёбо Мягкое нёбо Мягкое нёбо или нёбная...   \n",
       "2520494  Небо Небо Небо (производные небеса, небосвод) ...   \n",
       "2520495  Звёздное небо Звёздное небо Звёздное небо — со...   \n",
       "2520496  Диффузное излучение неба Диффузное излучение н...   \n",
       "2520497                                        \" Черновики   \n",
       "\n",
       "                                                      text  \n",
       "0                           Коcтюм женcкий. Новый комплект  \n",
       "1        Школьный коcтюм для мaльчикa 134. Школьный коc...  \n",
       "2        Иcлaмcкaя кapтинa из эпокcидной cмолы. Я pиcую...  \n",
       "3        Оpигинaл Nike Air Jordan 1 High OG University ...  \n",
       "4        Кpоccовки Nike Air Jordan 1 Low OG Travis Scot...  \n",
       "...                                                    ...  \n",
       "2520493  . Мягкое нёбо Мягкое нёбо Мягкое нёбо или нёбн...  \n",
       "2520494  . Небо Небо Небо (производные небеса, небосвод...  \n",
       "2520495  . Звёздное небо Звёздное небо Звёздное небо — ...  \n",
       "2520496  . Диффузное излучение неба Диффузное излучение...  \n",
       "2520497                                      . \" Черновики  \n",
       "\n",
       "[2520498 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df[\"text\"] = full_df[\"title\"] + \". \" + full_df[\"description\"]\n",
    "full_df.drop([\"title\", \"description\"], axis=1)\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05dbda55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cb80cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2520498/2520498 [01:13<00:00, 34417.99it/s]\n",
      "100%|██████████| 2520498/2520498 [01:05<00:00, 38725.92it/s]\n"
     ]
    }
   ],
   "source": [
    "text = full_df[\"text\"].tolist()\n",
    "\n",
    "corpus = []\n",
    "\n",
    "def preprocess(line):\n",
    "\tline = line.lower().replace(\"ё\", \"е\")\n",
    "\tline = re.sub(r\"\\s+\", \" \", line).strip()\n",
    "\treturn line\n",
    "\n",
    "\n",
    "for line in tqdm(text):\n",
    "\tcleaned_line = preprocess(line)\n",
    "\tif len(cleaned_line):\n",
    "\t\tcorpus.append(cleaned_line)\n",
    "\n",
    "with open(\"clean_corpus.txt\", \"w\", encoding=\"utf8\") as file:\n",
    "\tfor line in tqdm(corpus):\n",
    "\t\tfile.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a52403c",
   "metadata": {},
   "source": [
    "Поскольку датасет был для различия дубликатов объявлений, некоторые названия и описания повторяются чаще, чем следует, но сильно это не должно помешать. (но вообще мусора многовато осталось, на итоговом качестве это может сказаться)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29edcf93",
   "metadata": {},
   "source": [
    "Перейдем непосредственно к модели. Первоначально, для использования статистики по униграммам и биграммам в корпусе, нужно их посчитать, а потом для определения вероятностей использовать формулы:\n",
    "\n",
    "$$\n",
    "\tp(\\text{word}_2 | \\text{word}_1) = \\frac{p(\\text{word}_1, \\text{word}_2)}{p(\\text{word}_1)}\n",
    "$$\n",
    "\n",
    "где\n",
    "\n",
    "$$\n",
    "p(\\text{word}_i) = \\frac{n_{\\text{word}_i}}{n}, \\quad p(\\text{word}_i, \\text{word}_{i-1}) = \\frac{n_{\\text{word}_i, \\text{word}_{i-1}}}{n}\n",
    "$$\n",
    "\n",
    "тогда юзабельная формула:\n",
    "\n",
    "$$\n",
    "p(\\text{word}_i | \\text{word}_{i-1}) = \\frac{n_{\\text{word}_i, \\text{word}_{i-1}}}{n_{\\text{word}_{i-1}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13beb13",
   "metadata": {},
   "source": [
    "Перейдем к подсчетам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6acb8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3494545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2520498it [49:43, 844.80it/s]                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "количество слов: 867914283\n",
      "уникальных слов: 2205919\n",
      "уникальных биграмм: 14557985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unigram_counts = defaultdict(int)\n",
    "bigram_counts = defaultdict(int)\n",
    "total_words = 0\n",
    "\n",
    "with open(\"clean_corpus.txt\", \"r\", encoding='utf8') as file:\n",
    "    for line in tqdm(file, total=2_500_000):\n",
    "        tokens = [token.text for token in tokenize(line)]\n",
    "        \n",
    "        if not tokens:\n",
    "            continue\n",
    "            \n",
    "        total_words += len(tokens)\n",
    "        \n",
    "        for word in tokens:\n",
    "            unigram_counts[word] += 1\n",
    "\n",
    "        for i in range(len(tokens) - 1):\n",
    "            bigram = (tokens[i], tokens[i+1])\n",
    "            bigram_counts[bigram] += 1\n",
    "\n",
    "print(f\"количество слов: {total_words}\")\n",
    "print(f\"уникальных слов: {len(unigram_counts)}\")\n",
    "print(f\"уникальных биграмм: {len(bigram_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f476e950",
   "metadata": {},
   "source": [
    "*Как-то очень медленно с этим токенизатором все работает. Такое ощущение, что вручную написать код для разделения на токены было бы быстрее*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "id": "512ead5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 189 0 2\n"
     ]
    }
   ],
   "source": [
    "print(unigram_counts[\"виво\"], unigram_counts[\"сяоми\"], unigram_counts[\"ксяоми\"], unigram_counts[\"ксиоми\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d06b623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 13\n",
      "152 1\n"
     ]
    }
   ],
   "source": [
    "print(bigram_counts[(\"смартфон\", \"vivo\")], bigram_counts[(\"телефон\", \"vivo\")])\n",
    "print(bigram_counts[(\"хорошем\", \"состоянии\")], bigram_counts[(\"плохом\", \"состоянии\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f52f57",
   "metadata": {},
   "source": [
    "Видно, что датасет довольно маленький, чтобы с хорошей точностью восстановить статистику. Слова, которые точно должны встречаться среди запросов вместе, например \"смартфон vivo\", в датасет не входят, так что $p(\\text{\"смартфон\"} | \\text{\"vivo\"})$ будет строгим нулем вместо некоторого малого значения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987e5d6c",
   "metadata": {},
   "source": [
    "Теперь нужно придумать, как пользоваться полученными значениями эффективно. Для строки длины $l$ количество разбиений зависит от $l$ экспоненциально, так что просто перебор всех может оказаться слишком долгим. Нужно придумать нечто более эффективное, используя подход динамического программирования."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ffa39c",
   "metadata": {},
   "source": [
    "Решение:\n",
    "\n",
    "* Пусть мы рассматриваем префикс из $i$ первых букв. Тогда для определения лучшего его разбиения будем рассматривать все его суффиксы (буквы от некоторого $j$ до $i$) и считать скор разбиения с последним словом $[j:i]$. Для этого нужно знать скор лучшего разбиения префикса $[:j]$ и вероятность нового слова $[j:i]$ при условии последнего слова из лучшего разбиения префикса $[:j]$. Далее нужно будет выбрать лучшее разбиение по скору и сохранить его.\n",
    "\n",
    "* При этом, скор рассчитыватся, как\n",
    "$$\n",
    "score(x) := \\log p(x) = \\log p(x_1) + \\sum \\log p(x_i | x_{i-1})\n",
    "$$\n",
    "\n",
    "* Далее переходим к следующему символу и так же перебираем разбиения префикса.\n",
    "\n",
    "* Таким образом, у нас получается таблица со скорами лучшего разбиения всех префиксов строки и с помощью нее мы сможем восстановить это разбиение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6a69bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = {\n",
    "    \"unigram_counts\": dict(unigram_counts),\n",
    "    \"bigram_counts\": dict(bigram_counts),\n",
    "    \"total_words\": total_words,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "id": "b12ab4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_bonus(word_len):\n",
    "    return (word_len - 1) ** 2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "id": "02aea966",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter:\n",
    "\tdef __init__(self, model_data=model_data):\n",
    "\t\tself.unigrams = model_data[\"unigram_counts\"]\n",
    "\t\tself.bigrams = model_data[\"bigram_counts\"]\n",
    "\t\tself.total = model_data[\"total_words\"]\n",
    "\t\tself.log_total = np.log10(self.total)\n",
    "\t\tself.vocabular = set(self.unigrams.keys())\n",
    "\n",
    "\tdef unigram_score(self, word):\n",
    "\t\tcount = self.unigrams.get(word, 0)\n",
    "\t\tif count == 0:\n",
    "\t\t\treturn -np.inf\n",
    "\t\treturn np.log10(count) - self.log_total\n",
    "\n",
    "\tdef bigram_score(self, word_1, word_2):\n",
    "\t\tbigram_count = self.bigrams.get((word_1, word_2), 0)\n",
    "\t\tword1_count = self.unigrams.get(word_1, 0)\n",
    "\n",
    "\t\t#  если биграммы нет, считаем слова независимыми, тогда скор пары - это просто сумма скоров\n",
    "\t\tif bigram_count > 0 and word1_count > 0:\n",
    "\t\t\treturn np.log10(bigram_count) - np.log10(word1_count)\n",
    "\t\telse:\n",
    "\t\t\treturn self.unigram_score(word_1) + self.unigram_score(word_2)\n",
    "\n",
    "\tdef split(self, text):\n",
    "\t\tmem = {0: (0., 0)}\n",
    "\n",
    "\t\tfor i in range(1, len(text) + 1):\n",
    "\t\t\tbest_score = -np.inf\n",
    "\t\t\tbest_split_pos = 0\n",
    "\n",
    "\t\t\tfor j in range(max(0, i-20), i):\n",
    "\t\t\t\tword = text[j:i]\n",
    "\n",
    "\t\t\t\tif word in self.vocabular:\n",
    "\t\t\t\t\tprev_score, prev_best_split_pos = mem.get(j, (-np.inf, 0))\n",
    "\t\t\t\t\tif prev_score == -np.inf:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t\tprev_word = text[mem[j][1]:j] if j > 0 else None\n",
    "\n",
    "\t\t\t\t\tlog_prob = self.unigram_score(word) if prev_word is None else self.bigram_score(prev_word, word)\n",
    "\n",
    "\t\t\t\t\tnew_score = prev_score + log_prob + len_bonus(len(word))\n",
    "\n",
    "\t\t\t\t\tif new_score > best_score:\n",
    "\t\t\t\t\t\tbest_score = new_score\n",
    "\t\t\t\t\t\tbest_split_pos = j\n",
    "\n",
    "\t\t\tmem[i] = (best_score, best_split_pos)\n",
    "\n",
    "\t\tif mem[len(text)][0] == -np.inf:\n",
    "\t\t\treturn text\n",
    "\t\telse:\n",
    "\t\t\tresult = []\n",
    "\t\t\ti = len(text)\n",
    "\t\t\twhile i != 0:\n",
    "\t\t\t\tj = mem[i][1]\n",
    "\t\t\t\tresult.append(text[j:i])\n",
    "\t\t\t\ti = j\n",
    "\n",
    "\t\t\tresult.reverse()\n",
    "\t\t\treturn \" \".join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "id": "c1e27894",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = Splitter(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "id": "6969b95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "айфон 16 промах\n",
      "айфон 16 про макс\n",
      "айфон 1 8 про макс\n",
      "продам колесо недорого\n"
     ]
    }
   ],
   "source": [
    "print(splitter.split(\"айфон16промах\"))\n",
    "print(splitter.split(\"айфон16промакс\"))\n",
    "print(splitter.split(\"айфон18промакс\"))\n",
    "print(splitter.split(\"продамколесонедорого\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b1a084",
   "metadata": {},
   "source": [
    "Выглядит неплохо. Весов немало -- модель на 700МБ, зато запускается без гпу без проблем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "id": "291845d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_path = Path('bigram_model.pkl')\n",
    "with model_path.open('wb') as f:\n",
    "\tpickle.dump(model_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602c6969",
   "metadata": {},
   "source": [
    "Теперь практически все готово для разбиения предоставленного датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "id": "ecb3ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_map(text):\n",
    "    clean_chars = []\n",
    "    clean_to_original_map = []\n",
    "\n",
    "    for i, char in enumerate(text):\n",
    "        if not char.isspace():\n",
    "            clean_chars.append(char)\n",
    "            clean_to_original_map.append(i)\n",
    "\n",
    "    clean_text_for_splitter = \"\".join(clean_chars).lower().replace('ё', 'е')\n",
    "\n",
    "    return clean_text_for_splitter, clean_to_original_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "id": "db43faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_punctuation(text):\n",
    "\ttext = re.sub(r'\\s([?.!,:;\")])', r'\\1', text)\n",
    "\t\n",
    "\ttext = re.sub(r'([(\"])\\s', r'\\1', text)\n",
    "\n",
    "\ttext = re.sub(r'(\\S)-\\s', r'\\1 - ', text)\n",
    "\ttext = re.sub(r'\\s-(\\S)', r' - \\1', text)\n",
    "\t\n",
    "\treturn text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "id": "34c4479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce9558f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1005/1005 [00:00<00:00, 3239.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     text_no_spaces                        predicted_text\n",
      "14          ищурепетиторапобиологии            ищу репетитора п обиологии\n",
      "64           куплюхолодильникAtlant              куплю холодильник atlant\n",
      "128      сдампарковкувцентре,охрана        сдам парковку в центре, охрана\n",
      "343              МоскваIkeaдоставка                  москва ikea доставка\n",
      "471           Ядавлюсь,изрыгаяогонь           я давлю сь, из ры га яогонь\n",
      "520            ХочешьзаокошкомАльпы              хочешь з а окошком альпы\n",
      "647   Чтобыбылидрузьяили,хотябыодин    чтобы были друзья или, хотябы один\n",
      "812   Годыпрошли,Городецкийнетужит,   годы прошли, город ецкий н е тужит,\n",
      "838       Какие-толюдитебяокружают,          какие-то люди тебя окружают,\n",
      "856  Ивдомелесникаяночлегапопросил.  и вдоме лесника я ночлег а попросил.\n",
      "      id      predicted_positions\n",
      "14    14              [3, 13, 14]\n",
      "64    64                  [5, 16]\n",
      "128  128          [4, 12, 13, 20]\n",
      "343  343                  [6, 10]\n",
      "471  471    [1, 6, 9, 11, 13, 15]\n",
      "520  520            [6, 7, 8, 15]\n",
      "647  647       [5, 9, 15, 19, 25]\n",
      "812  812  [4, 11, 16, 21, 22, 23]\n",
      "838  838              [8, 12, 16]\n",
      "856  856   [1, 6, 13, 14, 20, 21]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "task_data = pd.read_csv(\"dataset.txt\", sep=\";\", engine=\"python\")\n",
    "\n",
    "all_predicted_positions = []\n",
    "all_predicted_texts = []\n",
    "\n",
    "for text_no_spaces in tqdm(task_data['text_no_spaces']):\n",
    "\tclean_text, mapping = preprocess_and_map(text_no_spaces)\n",
    "\t\n",
    "\tif not clean_text:\n",
    "\t\tall_predicted_texts.append(text_no_spaces)\n",
    "\t\tcontinue\n",
    "\n",
    "\traw_split_text = splitter.split(clean_text)\n",
    "\n",
    "\tfinal_split_text = post_process_punctuation(raw_split_text)\n",
    "\tall_predicted_texts.append(final_split_text)\n",
    "\n",
    "\ttokens = final_split_text.split(' ')\n",
    "\t\n",
    "\tlengths = [len(token) for token in tokens[:-1]]\n",
    "    \n",
    "\tpositions = list(map(str, itertools.accumulate(lengths)))\n",
    "\n",
    "\tpositions_str = \"[\" + \", \".join(positions) + \"]\"\n",
    "\tall_predicted_positions.append(positions_str)\n",
    "\n",
    "\n",
    "submission_df = task_data.copy()\n",
    "submission_df[\"predicted_text\"] = all_predicted_texts\n",
    "submission_df[\"predicted_positions\"] = all_predicted_positions\n",
    "answers_df = submission_df[[\"id\", \"predicted_positions\"]].copy()\n",
    "\n",
    "idxs = np.random.randint(0, 1000, 10)\n",
    "print(submission_df[[\"text_no_spaces\", \"predicted_text\"]][submission_df[\"id\"].isin(idxs)])\n",
    "print(answers_df[submission_df[\"id\"].isin(idxs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52df93bb",
   "metadata": {},
   "source": [
    "*Как для модели на биграммах, которая работает без гпу и чего-то сложного, получилось неплохо, но от идеала очень далеко. Сказываются и артефакты датасета, на котором обучалась модель. Довольно часто можно встретить неправильные слова, при этом некоторых специфических слов, редких, зачастую не находится. Для лучшего качества, и тем более, для использования триграмм следует найти более обширный датасет.*\n",
    "\n",
    "*Кроме того, для улучшения можно попробовать результаты уже этой модели подавать на вход некоторой нейросетевой, которая может хранить долгий контекст, и с хорошим токенайзером. В таком случае разбиение на токены будет уже более осмысленным в отличие от разбиения на токены текста без пробелов. Однако, тогда потеряется смысл такой простой модели, на биграммах, который заключается в скорости инференса и низких требований к устройству, а для этой задачи, кажется, использование модели со сложной архитектурой излишне. С таким же успехом можно скормить слитные тексты гпт с промптом и она их отлично разделит. Лучший результат можно достигнуть за счет более тщательного подбора и очистки датасета и использования трехграмм. Хотя более продвинутые, но все еще легковесные подходы могут показать результат лучше.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "id": "b879f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_df.to_csv(\"answers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813b6ab7",
   "metadata": {},
   "source": [
    "92% f1 score, не густо. Надо попробовать улучшить это.\n",
    "\n",
    "Хороший датасет, подходящий для данной задачи, еще и так, чтобы обучение не занимало много времени, найти сложно. Так что будем использовать предобученную модель. Будем использовать микроберт на 12М весов, обученный на русском разговорном, а именно https://huggingface.co/DeepPavlov/distilrubert-tiny-cased-conversational-v1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "id": "dfaad608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.56.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hxrt mx\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "id": "66ab3414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a495b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyBertCorrector(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.bert = AutoModel.from_pretrained(\"DeepPavlov/distilrubert-tiny-cased-conversational-v1\")\n",
    "\n",
    "\t\tfor param in self.bert.parameters():\n",
    "\t\t\tparam.require_grad = False\n",
    "\n",
    "\t\tself.hiddend_size = 264\n",
    "\t\tself.classifier = nn.Sequential(\n",
    "\t\t\tnn.Linear(self.hiddend_size, 256),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(0.2),\n",
    "\t\t\tnn.Linear(256, 1),\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, tensor):\n",
    "\t\treturn self.classifier(self.bert(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "id": "a1c8c0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\hxrt mx\\.cache\\huggingface\\hub\\models--DeepPavlov--distilrubert-tiny-cased-conversational-v1\\snapshots\\2033d0d1de807e8181ebfa0e53d2a8e526412b0f\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 264,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 792,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"transformers_version\": \"4.56.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\hxrt mx\\.cache\\huggingface\\hub\\models--DeepPavlov--distilrubert-tiny-cased-conversational-v1\\snapshots\\2033d0d1de807e8181ebfa0e53d2a8e526412b0f\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\hxrt mx\\.cache\\huggingface\\hub\\models--DeepPavlov--distilrubert-tiny-cased-conversational-v1\\snapshots\\2033d0d1de807e8181ebfa0e53d2a8e526412b0f\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\hxrt mx\\.cache\\huggingface\\hub\\models--DeepPavlov--distilrubert-tiny-cased-conversational-v1\\snapshots\\2033d0d1de807e8181ebfa0e53d2a8e526412b0f\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\hxrt mx\\.cache\\huggingface\\hub\\models--DeepPavlov--distilrubert-tiny-cased-conversational-v1\\snapshots\\2033d0d1de807e8181ebfa0e53d2a8e526412b0f\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 264,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 792,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"transformers_version\": \"4.56.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\hxrt mx\\.cache\\huggingface\\hub\\models--DeepPavlov--distilrubert-tiny-cased-conversational-v1\\snapshots\\2033d0d1de807e8181ebfa0e53d2a8e526412b0f\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 264,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 792,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"transformers_version\": \"4.56.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/distilrubert-tiny-cased-conversational-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6e4a1d",
   "metadata": {},
   "source": [
    "Для обучения на нашем корпусе потребуется сделать из него набор токенов с метками, ставиться ли пробел между ними, или нет. Для этого будем использовать более правильный с точки зрения пунктуации корпус -- open corpora. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946f5a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 655/4007 [00:14<00:22, 150.78it/s]"
     ]
    }
   ],
   "source": [
    "corpus = additional_df[\"description\"].tolist()\n",
    "\n",
    "correct_tokens = []\n",
    "bigram_model_out_tokens = []\n",
    "\n",
    "for text in tqdm(corpus):\n",
    "\tpreprocess_text = preprocess(text)\n",
    "\tbigram_model_out = splitter.split(preprocess_text)\n",
    "\t\n",
    "\tbigram_model_out_tokens.append(tokenizer.tokenize(bigram_model_out))\n",
    "\tcorrect_tokens.append(tokenizer.tokenize(preprocess_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "id": "d191bab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram_model out tokens: ['\"', 'частный', 'корреспон', '##дент', '\"']\n",
      "correct tokens: ['\"', 'Ча', '##стный', 'корреспон', '##дент', '\"']\n"
     ]
    }
   ],
   "source": [
    "print(f\"bigram_model out tokens: {bigram_model_out_tokens[0]}\")\n",
    "print(f\"correct tokens: {correct_tokens[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113dc02a",
   "metadata": {},
   "source": [
    "Теперь нужно научить наш микроберт с классификатором на голове для каждой пары соседних токенов предсказывать: должен там стоять пробел, или нет."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
